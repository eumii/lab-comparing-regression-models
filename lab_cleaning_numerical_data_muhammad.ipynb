{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first lab: CLEANING NUMERICAL DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### displaying main features (head, shape, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'we_fn_use_c_marketing_customer_value_analysis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25804\\1882739168.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcustomer_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"we_fn_use_c_marketing_customer_value_analysis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emmanuel\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'we_fn_use_c_marketing_customer_value_analysis.csv'"
     ]
    }
   ],
   "source": [
    "customer_df = pd.read_csv(\"we_fn_use_c_marketing_customer_value_analysis.csv\")\n",
    "display(customer_df.head())\n",
    "display(customer_df.shape)\n",
    "display(customer_df.info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### snake case label and formatting datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[]\n",
    "for i in customer_df.columns:\n",
    "    cols.append(i.lower())\n",
    "customer_df.columns = cols\n",
    "customer_df.columns = customer_df.columns.str.replace(' ', '_')\n",
    "customer_df = customer_df.rename(columns = {\"employmentstatus\":\"employment_status\"})\n",
    "customer_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting max columns bcs it looks better and there are only 24 columns, it wouldn't hurt, I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turning column effective_to_date into datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df[\"effective_to_date\"] = customer_df[\"effective_to_date\"].astype(\"datetime64[ns]\")\n",
    "type(customer_df[\"effective_to_date\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### separating continuous and discrete columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first: separate all numerical columns from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = customer_df.select_dtypes(include=np.number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then see which column(s) contains how many unique values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to put columns with up to 200s unique values to our discrete_df and the rest to continuous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is me reviewing python fundamentals before making the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning how to get the count of unique values\n",
    "# learning how to turn something to a list\n",
    "# (numerical_df.nunique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esraa sent me this. forgot to ask what this line was for\n",
    "# for col in data.columns:\n",
    "#     print (len(data[col].value_counts()),'        ',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning how to create a new dataframe\n",
    "# a = pd.DataFrame()\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning pandas indexing\n",
    "# print(numerical_df.iloc[:,0])\n",
    "# numerical_df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Erin\n",
    "# for i in data_num.columns:\n",
    "#     if len(data_num[i].unique()) > 300:\n",
    "#         continuous_lst.append(i)\n",
    "#     else:\n",
    "#         discrete_lst.append(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making function to split numerical_df into discrete and cont, based on the count of unique values of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_dis(df):\n",
    "    con = pd.DataFrame()\n",
    "    dis = pd.DataFrame()\n",
    "    for i in range(len(df.nunique().tolist())):\n",
    "        if df.nunique().tolist()[i] > 250:\n",
    "            con[df.columns[i]] = df.iloc[:,i]\n",
    "        else:\n",
    "            dis[df.columns[i]] = df.iloc[:,i]\n",
    "    return con, dis\n",
    "\n",
    "continuous_df, discrete_df = con_dis(numerical_df)\n",
    "display(discrete_df.nunique())\n",
    "display(continuous_df.nunique())\n",
    "display(discrete_df.head())\n",
    "display(continuous_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### heatmap and correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "correlations_matrix = customer_df.corr()\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theres some correlations, one of the highest would be between total_claim_amount and monthly_premium_auto\n",
    "# if we are about to build models, we might build it without dropping those correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plotting the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_df.columns:\n",
    "    sns.displot(x = i, data = discrete_df, bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as for income, we could make bins\n",
    "# for number_of_open_complaints we can leave it as it is\n",
    "# for number_of_policies, we might want to group 4-9 into one category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_df.columns:\n",
    "    sns.histplot(x = i, data = continuous_df, bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment what you see in the plots\n",
    "# will do it later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### looking for outliers: use boxplots and/or others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_df.columns:\n",
    "    sns.histplot(x = i, data = discrete_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_df.columns:\n",
    "    sns.boxplot(x = i, data = continuous_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot seems better for the continuous_df, while\n",
    "# it doesnt bring us much information with the discrete_df\n",
    "# for discrete_df we find histplot a better option to see the outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the outliers: we're probably gonna drop them. but it depends on the variable and the values inside them. we can either use mean, mode, or drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no NaNs. nice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next lab: CLEANING CATEGORICAL DATA "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### defining categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = customer_df.select_dtypes(include=[np.object])\n",
    "categorical_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no NaNs. nice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check all unique values in all columns, to decide what to do with each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_df.columns:\n",
    "    print(\"column\", i+\":\")\n",
    "    display(categorical_df[i].value_counts(dropna = False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the columns might need to be changed into binaries (?), like\n",
    "instead of F and M in gender, we could change it into 1 for female and 0 for male"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we might use onehot to encode almost all the columns here except customer. either exclude it so it doesnt get onehot,\n",
    "or just drop the whole column customer, since the values are all-unique. we won't (probably) get any significant informations from an all-unique column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could try seeing the correlation matrix to check whether the subcategories (L1 - L3) from the column policy have\n",
    "collinearity with other columns. if they show muilticollinearity, we could drop the column policy and keep just the \n",
    "policy type instead. (will do the corr_matrix later/tomorrow/some other time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the policy/policy-type, either one needs to be dropped, since it just repeats the information. most probably the column policy has some multicollinearity with other columns so we might want to drop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as for renew_offer_type, it doesnt hurt to keep it. unless we find multicollinearity in heatmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuing our \"what we could do\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combine the value master and doctor as a \"higher educational degree\"\n",
    "- combine college with bachelor as \"college\" or some other name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make 1 bigger category consists of luxury cars, luxury suv, and sport cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combine medical leave with employed. or search the definition about it. but combining them sounds nice\n",
    "- might combine medical_leave and disabled into 1 new category\n",
    "- might combine retired with unemployed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### executing the plan(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the whole column \"customer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    categorical_df = categorical_df.drop([\"customer\"], axis = 1)\n",
    "    print(\"deleted the column.\")\n",
    "except:\n",
    "    print(\"column already deleted.\")\n",
    "categorical_df.head(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the whole column \"policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    categorical_df = categorical_df.drop([\"policy\"], axis = 1)\n",
    "    print(\"deleted the column.\")\n",
    "except:\n",
    "    print(\"column already deleted.\")\n",
    "categorical_df.head(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefining unique values we want to keep in column \"vehicle_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df['vehicle_class'] = np.where(categorical_df.vehicle_class.isin([\"Sports Car\",\"Luxury SUV\",\"Luxury Car\"]),'Luxury Cars', categorical_df.vehicle_class)\n",
    "categorical_df['vehicle_class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefining unique values we want to keep in column \"education\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_education(x):\n",
    "    if x in [\"Master\", \"Doctor\"]:\n",
    "        return 'higher educational degree'\n",
    "    elif x in [\"Bachelor\", \"College\"]:\n",
    "        return \"baccalaureate degree\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "categorical_df['education'] = categorical_df['education'].apply(clean_education)\n",
    "categorical_df['education'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefining unique values we want to keep in column \"employment_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df['employment_status'] = np.where(categorical_df.employment_status.isin([\"Medical Leave\",\"Disabled\"]),'M-D_related_leave', categorical_df.employment_status)\n",
    "categorical_df['employment_status'] = np.where(categorical_df.employment_status.isin([\"Retired\"]),'Unemployed', categorical_df.employment_status)\n",
    "categorical_df['employment_status'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just listing what's done here\n",
    "- dropped the column \"customer\"\n",
    "- dropped column \"policy\"\n",
    "- simplified \"education\" column\n",
    "- simplified \"vehicle_class\" column\n",
    "- simplified \"employment_status\" column\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next lab: FEATURE EXTRACTION "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plotting something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_df.columns:\n",
    "    sns.histplot(data = categorical_df[i])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plotting datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just checking what the datetime column name was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "sns.lineplot(data=customer_df, x=\"effective_to_date\", y='total_claim_amount',markers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### simao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see simao's plotting. it looks coool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next lab: COMPARING REGRESSION MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_df.columns:\n",
    "    sns.boxplot(x = i, data = continuous_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_upper_outliers(df, column, outlier):\n",
    "    df = df[df[column] < outlier]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_customer_df = customer_df.copy()\n",
    "c_customer_df = drop_upper_outliers(c_customer_df, \"total_claim_amount\", 2200)\n",
    "c_customer_df = drop_upper_outliers(c_customer_df, \"customer_lifetime_value\", 54000)\n",
    "c_customer_df = drop_upper_outliers(c_customer_df, \"monthly_premium_auto\", 210)\n",
    "c_customer_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_df.columns:\n",
    "    sns.histplot(x = i, data = discrete_df)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im so confused here, do I need to redefine the categoricals again? since the number of rows has decreased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### redefining categoricals, numericals, discrete, continuous (again?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_dis(df):\n",
    "    con = pd.DataFrame()\n",
    "    dis = pd.DataFrame()\n",
    "    for i in range(len(df.nunique().tolist())):\n",
    "        if df.nunique().tolist()[i] > 250:\n",
    "            con[df.columns[i]] = df.iloc[:,i]\n",
    "        else:\n",
    "            dis[df.columns[i]] = df.iloc[:,i]\n",
    "    return con, dis\n",
    "\n",
    "categorical_df = c_customer_df.select_dtypes(include=[np.object])\n",
    "numerical_df = c_customer_df.select_dtypes(include=np.number)\n",
    "continuous_df, discrete_df = con_dis(numerical_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redo all the \"cleaning categorical\" for the new df (after removing outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column \"customer\"\n",
    "try:\n",
    "    categorical_df = categorical_df.drop([\"customer\"], axis = 1)\n",
    "    print(\"deleted the column.\")\n",
    "except:\n",
    "    print(\"column already deleted.\")\n",
    "categorical_df.head(0)\n",
    "\n",
    "# drop column \"policy\"\n",
    "try:\n",
    "    categorical_df = categorical_df.drop([\"policy\"], axis = 1)\n",
    "    print(\"deleted the column.\")\n",
    "except:\n",
    "    print(\"column already deleted.\")\n",
    "categorical_df.head(0)\n",
    "\n",
    "# reducing unique values from \"vehicle_class\"\n",
    "categorical_df['vehicle_class'] = np.where(categorical_df.vehicle_class.isin([\"Sports Car\",\"Luxury SUV\",\"Luxury Car\"]),'Luxury Cars', categorical_df.vehicle_class)\n",
    "\n",
    "# reducing unique values from \"education\"\n",
    "def clean_education(x):\n",
    "    if x in [\"Master\", \"Doctor\"]:\n",
    "        return 'higher educational degree'\n",
    "    elif x in [\"Bachelor\", \"College\"]:\n",
    "        return \"baccalaureate degree\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "categorical_df['education'] = categorical_df['education'].apply(clean_education)\n",
    "\n",
    "# reducing unique values in \"employment_status\"\n",
    "categorical_df['employment_status'] = np.where(categorical_df.employment_status.isin([\"Medical Leave\",\"Disabled\"]),'M-D_related_leave', categorical_df.employment_status)\n",
    "categorical_df['employment_status'] = np.where(categorical_df.employment_status.isin([\"Retired\"]),'Unemployed', categorical_df.employment_status)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### concat discrete and continuous as numerical; concat numerical and categorical as data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i decided to standardize both discrete and continuous because if I dont, the continuous will have a way smaller value range, while discrete still has a wider value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_new = pd.concat([discrete_df, continuous_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([numerical_new, categorical_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# clean data, dropping any rows and columns that need to be dropped (1)\n",
    "# select features (could postpone this)\n",
    "# X/y split\n",
    "# Train/Test split\n",
    "# split both Train and Test in numericals and categoricals\n",
    "# transformations on numericals:\n",
    "#     fit ONLY on numericals_train\n",
    "#     transform BOTH numericals_train and numericals_test\n",
    "# encoding categoricals\n",
    "#     fit ONLY on categricals_train\n",
    "#     encode BOTH categoricals_train and categoricals_test\n",
    "# combine numericals_train and categoricals_train into train_processed\n",
    "# combine numericals_test and categoricals_test into test_processed\n",
    "# define model\n",
    "# fit model on train_processed\n",
    "# evaluate (score) model on test_processed + sanity check (does it make sense?)\n",
    "# save model and transformers/encoders (2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### X-y-split, then train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[[\"total_claim_amount\"]].copy()\n",
    "X = data.drop(columns = \"total_claim_amount\", axis = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### splitting each train and test set to numericals and categoricals, then do scaling and/or encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_train = X_train.select_dtypes(np.number)\n",
    "numericals_test = X_test.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "transformer = MinMaxScaler().fit(numericals_train)\n",
    "numericals_train_standardized = transformer.transform(numericals_train)\n",
    "numericals_test_standardized = transformer.transform(numericals_test)\n",
    "numericals_train_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals_train = X_train.select_dtypes(object)\n",
    "categoricals_test = X_test.select_dtypes(object)\n",
    "display(categoricals_train.shape)\n",
    "display(categoricals_train.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = [\"coverage\",\"employment_status\",\"location_code\",\"vehicle_size\"]\n",
    "onehot_cols = [\"state\",\"marital_status\",\"policy_type\",\"renew_offer_type\",\"sales_channel\",\"vehicle_class\"]\n",
    "cat_ordinal_train = pd.DataFrame(categoricals_train, columns = ordinal_cols)\n",
    "cat_ordinal_test = pd.DataFrame(categoricals_test, columns = ordinal_cols)\n",
    "cat_onehot_train = pd.DataFrame(categoricals_train, columns = onehot_cols)\n",
    "cat_onehot_test = pd.DataFrame(categoricals_test, columns = onehot_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown = 'error', drop = 'first').fit(cat_onehot_train)\n",
    "onehot_train_encoded = encoder.transform(cat_onehot_train).toarray()\n",
    "onehot_test_encoded = encoder.transform(cat_onehot_test).toarray()\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_encoder = OrdinalEncoder(handle_unknown= \"error\").fit(cat_ordinal_train)\n",
    "ordinal_train_encoded = ord_encoder.transform(cat_ordinal_train)\n",
    "ordinal_test_encoded = ord_encoder.transform(cat_ordinal_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### final step: concat everything to processed X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals_train_encoded = np.concatenate((onehot_train_encoded, ordinal_train_encoded), axis = 1)\n",
    "categoricals_test_encoded = np.concatenate((onehot_test_encoded, ordinal_test_encoded), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = np.concatenate((numericals_train_standardized,categoricals_train_encoded),axis=1)\n",
    "X_test_processed = np.concatenate((numericals_test_standardized,categoricals_test_encoded),axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importing libraries and defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models_and_get_scores(xtrain, xtest, ytrain, ytest):\n",
    "    model = []\n",
    "    train = []\n",
    "    test = []\n",
    "    test_result = pd.DataFrame()\n",
    "\n",
    "    KNN = KNeighborsRegressor(n_neighbors=5, p = 1)\n",
    "    KNN.fit(xtrain, ytrain)\n",
    "    model.append(\"K-Neighbors\")\n",
    "    KNN_R2_score_train = KNN.score(xtrain, ytrain)\n",
    "    train.append(KNN_R2_score_train)\n",
    "    KNN_R2_score_test = KNN.score(xtest, ytest)\n",
    "    test.append(KNN_R2_score_test)\n",
    "    # KNN_predictions_train = KNN.predict(xtrain)\n",
    "    # KNN_predictions_test = KNN.predict(xtest)\n",
    "\n",
    "    lm = linear_model.LinearRegression()\n",
    "    lm.fit(xtrain,ytrain)\n",
    "    model.append(\"Linear Regression\")\n",
    "    LM_predictions_train = lm.predict(xtrain)\n",
    "    LM_R2_score_train = r2_score(ytrain, LM_predictions_train)\n",
    "    train.append(LM_R2_score_train)\n",
    "    LM_predictions_test = lm.predict(xtest)\n",
    "    LM_R2_score_test = r2_score(ytest, LM_predictions_test)\n",
    "    test.append(LM_R2_score_test)\n",
    "\n",
    "    regr = MLPRegressor(random_state = 1, max_iter = 700).fit(xtrain, ytrain)\n",
    "    model.append(\"MLP Regressor\")\n",
    "    mlp_score_train = regr.score(xtrain, ytrain)\n",
    "    train.append(mlp_score_train)\n",
    "    mlp_score_test = regr.score(xtest, ytest)\n",
    "    test.append(mlp_score_test)\n",
    "    # mlp_predictions_train = regr.predict(xtrain)\n",
    "    # mlp_predictions_test = regr.predict(xtest)\n",
    "\n",
    "    test_result[\"model\"] = model\n",
    "    test_result[\"r2_train\"] = train\n",
    "    test_result[\"r2_test\"] = test\n",
    "\n",
    "    return test_result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run the function to get test results in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = make_models_and_get_scores(X_train_processed, X_test_processed, y_train, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discussing the result\n",
    "# of the 3 models, MLP regressor seem to show the best scores both in train and test\n",
    "# linreg scores poorly :(\n",
    "# KNN scores almost as good as MLP, but MLP works a lot slower as the max_iter gets higher\n",
    "# MLP scores are, up to certain point, related to the number of max_iter\n",
    "# after max_iter = 700, the test score doesnt get much higher anymore than shown above\n",
    "# but with max_iter = 700 the MLP calculation takes more time than KNN\n",
    "# so if we're too lazy to wait, or if we have bigger dataframe and want to experiment iteratively,\n",
    "# MLP might not be too desirable as it takes a lot of time, and even more if you're looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b3bd93e6e993b5a0362fec8f7339ef1ba3066e57f882c75a84b879c84a67f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
